{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":63056,"databundleVersionId":9094797,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deep Smote\ndeep synthetic minority oversampling ","metadata":{}},{"cell_type":"markdown","source":"### 1. Intial sizes of images\nFind the images are in exact pixel sizes or ","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O\nimport os\nfrom PIL import Image\nfrom sklearn.neighbors import NearestNeighbors\nfrom PIL import Image, ImageOps\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom __future__ import print_function\n#%matplotlib inline\nimport argparse\nimport os\nimport time\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seed for reproducibility\nmanualSeed = 999\n#manualSeed = random.randint(1, 10000) # use if you want new results\nprint(\"Random Seed: \", manualSeed)\nrandom.seed(manualSeed)\ntorch.manual_seed(manualSeed)\n\n# image directory\nimage_dir = '/kaggle/input/isic-2024-challenge/train-image/image'\n# get 10 images\nimage_files = os.listdir(image_dir)[:10]\n\nfor i, file_name in enumerate(image_files, 1):\n    # iterate over the image files that get and\n    # join the path to open each image\n    image_path = os.path.join(image_dir, file_name)\n    with Image.open(image_path) as img:\n        print(f\"Image {i}: {file_name} - Size: {img.size}\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T14:37:22.012648Z","iopub.execute_input":"2025-06-29T14:37:22.013110Z","iopub.status.idle":"2025-06-29T14:37:33.900913Z","shell.execute_reply.started":"2025-06-29T14:37:22.013086Z","shell.execute_reply":"2025-06-29T14:37:33.900205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"meta_dir = \"/kaggle/input/isic-2024-challenge/\"\n\n# read the csv file\ndf = pd.read_csv(meta_dir + \"train-metadata.csv\")\n\n# display head\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T14:38:14.578571Z","iopub.execute_input":"2025-06-29T14:38:14.578991Z","iopub.status.idle":"2025-06-29T14:38:21.475672Z","shell.execute_reply.started":"2025-06-29T14:38:14.578968Z","shell.execute_reply":"2025-06-29T14:38:21.474909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for training the encoder we need only positive cases\n# so others are drop by the dataframe\ndf_filtered = df[df[\"target\"] == 1].reset_index(drop=True)\n\n# show the new data frame\ndf_filtered.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T14:38:30.404008Z","iopub.execute_input":"2025-06-29T14:38:30.404638Z","iopub.status.idle":"2025-06-29T14:38:30.426402Z","shell.execute_reply.started":"2025-06-29T14:38:30.404612Z","shell.execute_reply":"2025-06-29T14:38:30.425823Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Sample view on the malignant skin images","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# get first 10 isic_ids as sample and view them \nisic_ids = df_filtered['isic_id'].head(10)\n\n# plot the images\nplt.figure(figsize=(15, 6))\nfor i, isic_id in enumerate(isic_ids):\n    image_path = os.path.join(image_dir, f\"{isic_id}.jpg\")\n    img = Image.open(image_path).convert(\"RGB\")\n    \n    plt.subplot(2, 5, i + 1)\n    plt.imshow(img)\n    plt.title(isic_id)\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T14:38:33.607109Z","iopub.execute_input":"2025-06-29T14:38:33.607416Z","iopub.status.idle":"2025-06-29T14:38:34.815189Z","shell.execute_reply.started":"2025-06-29T14:38:33.607394Z","shell.execute_reply":"2025-06-29T14:38:34.814197Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. Image resize to 128*128 size images\n\nImages are in different image sizes so this different image sizes are made to same size images","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\n\nimage_dir = '/kaggle/input/isic-2024-challenge/train-image/image'\nsave_dir = '/kaggle/working/resized_images'\nos.makedirs(save_dir, exist_ok=True)\n\n# get sample 10 images to test\nisic_ids = df_filtered['isic_id'].head(10)\n\n# plot resized images\nplt.figure(figsize=(15, 6))\nfor i, isic_id in enumerate(isic_ids):\n    image_path = os.path.join(image_dir, f\"{isic_id}.jpg\")\n    img = Image.open(image_path).convert(\"RGB\")\n\n    # fit image to 128x128\n    img_resized = ImageOps.fit(img, (128, 128), method=Image.BICUBIC)\n\n    # show the size of the image\n    print(f\"{isic_id}: {img_resized.size}\")\n    \n    # save resized image\n    save_path = os.path.join(save_dir, f\"{isic_id}.jpg\")\n    img_resized.save(save_path)\n    \n    # show image in plot \n    plt.subplot(2, 5, i + 1)\n    plt.imshow(img_resized)\n    plt.title(isic_id)\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T14:38:43.538146Z","iopub.execute_input":"2025-06-29T14:38:43.538481Z","iopub.status.idle":"2025-06-29T14:38:44.456636Z","shell.execute_reply.started":"2025-06-29T14:38:43.538456Z","shell.execute_reply":"2025-06-29T14:38:44.455828Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check for specific classes \n# there are classes in the malignant data so we get the distinct cases \nunique_classes = df_filtered['iddx_3'].unique()\nprint(\"Distinct classes in 'iddx_3':\", unique_classes)\nprint(\"Distinct classes\", len(unique_classes))\n\n# count occurrences of each class\nclass_counts = df_filtered['iddx_3'].value_counts()\nprint(\"Class counts in 'iddx_3':\\n\", class_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T14:38:49.187309Z","iopub.execute_input":"2025-06-29T14:38:49.188082Z","iopub.status.idle":"2025-06-29T14:38:49.197050Z","shell.execute_reply.started":"2025-06-29T14:38:49.188056Z","shell.execute_reply":"2025-06-29T14:38:49.196307Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. Sample generation code from the k-nearest \n\ngenerate samples from the data \n1. get random sample from the data (base points from the dataset)\n2. for each data point, randomly choose one of its k neighbors (excluding itself)\n3. get the real vector for each base points and the neighbours\n4. generate new points by interpolating between each base and its chosen neighbor.\n\nThere are 8 different malignant() cases for that we need to generate seperate generation for each different cases.","metadata":{}},{"cell_type":"code","source":"# sample data of x\nX = np.random.randn(10,6)\nX.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T14:38:53.400073Z","iopub.execute_input":"2025-06-29T14:38:53.400432Z","iopub.status.idle":"2025-06-29T14:38:53.406063Z","shell.execute_reply.started":"2025-06-29T14:38:53.400391Z","shell.execute_reply":"2025-06-29T14:38:53.405257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X data size is the (number_of_dataset, n_vector_space)\n# n_to_sample number of samples to generate\n# cl means number of class the index of the class\ndef G_SM1(X, y,n_to_sample,cl):\n    \n    # if the incoming x data is more than 5 samples\n    if (X.shape[0] > 5):\n        n_neigh = 5 + 1\n    else:\n        # if not get the neighbors as available points\n        n_neigh = X.shape[0]\n        \n    # initialize the model with the NearestNeighbors\n    nn = NearestNeighbors(n_neighbors=n_neigh, n_jobs=1)\n    nn.fit(X)\n    \n    # Returns indices of neighbors of each point\n    # ind means the nearest neighbours to each point in\n    dist, ind = nn.kneighbors(X)\n    ##########\n    # ind is the indexes of nearest neighbors that are close to the current index\n    ##########\n    \n    # generating samples\n    # get random index list from the data\n    # base_indices that are get for generating new samples\n    base_indices = np.random.choice(list(range(len(X))),n_to_sample)\n\n    # considered neighbors that are most nearest neighbours\n    neighbor_indices = np.random.choice(list(range(1, n_neigh)),n_to_sample)\n\n    # get the correct data from the x    \n    X_base = X[base_indices]\n    X_neighbor = X[ind[base_indices, neighbor_indices]]\n\n    # calculate samples based on the random value \n    # interpolate the vectors\n    samples = X_base + np.multiply(np.random.rand(n_to_sample,1),\n            X_neighbor - X_base)\n\n    # return the vectors size: (n_to_sample x n_vector_space)\n    # and there corresponding labels size: (n_to_sample)\n    # return class and the label\n    return samples, [cl]*n_to_sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T14:38:55.282900Z","iopub.execute_input":"2025-06-29T14:38:55.283552Z","iopub.status.idle":"2025-06-29T14:38:55.289475Z","shell.execute_reply.started":"2025-06-29T14:38:55.283526Z","shell.execute_reply":"2025-06-29T14:38:55.288696Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"interpolated_sample, y = G_SM1(X, 1, 5, 0)\n# X size is (10 x 6)\n# we made the interpolation with the nearest points\n# made the sample of 5 generated\n# so size become (5 x 6)\nprint(interpolated_sample)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T14:38:59.237907Z","iopub.execute_input":"2025-06-29T14:38:59.238416Z","iopub.status.idle":"2025-06-29T14:38:59.281966Z","shell.execute_reply.started":"2025-06-29T14:38:59.238393Z","shell.execute_reply":"2025-06-29T14:38:59.281292Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5. Make training dataset CSV\n\nwe make the training dataset with all the negative cases and the 10000 of positive cases.","metadata":{}},{"cell_type":"code","source":"# filter malignant (target == 1)\ndf_positive = df[df[\"target\"] == 1]\n\n# filter benign (target == 0), randomly sample 10,000 rows\ndf_negative = df[df[\"target\"] == 0].sample(n=10000, random_state=42)\n\n# combine both positive and negative cases\ndf_balanced = pd.concat([df_positive, df_negative]).reset_index(drop=True)\n\n# shuffle the data\ndf_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# save to a new CSV\ndf_balanced.to_csv(\"/kaggle/working/train_created.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T14:39:03.787682Z","iopub.execute_input":"2025-06-29T14:39:03.788242Z","iopub.status.idle":"2025-06-29T14:39:04.420950Z","shell.execute_reply.started":"2025-06-29T14:39:03.788220Z","shell.execute_reply":"2025-06-29T14:39:04.420376Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_balanced.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T14:39:07.450133Z","iopub.execute_input":"2025-06-29T14:39:07.450444Z","iopub.status.idle":"2025-06-29T14:39:07.467002Z","shell.execute_reply.started":"2025-06-29T14:39:07.450423Z","shell.execute_reply":"2025-06-29T14:39:07.466281Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.1 Make a tranformation and see the data (64x64)\n\nTranformation to 64 by 64 reduce the computational cost ","metadata":{}},{"cell_type":"code","source":"# transformation\nresize_transform = transforms.Resize((64, 64))\n# image dir\nimage_dir = '/kaggle/input/isic-2024-challenge/train-image/image'\n\n# get sample 10 images to test\nisic_ids = df_filtered['isic_id'].head(10)\n\n# plot resized images\nplt.figure(figsize=(12, 5))\nfor i, isic_id in enumerate(isic_ids):\n    image_path = os.path.join(image_dir, f\"{isic_id}.jpg\")\n    img = Image.open(image_path).convert(\"RGB\")\n\n    # apply resize transform\n    img_resized = resize_transform(img)\n    print(f\"{isic_id}: {img_resized.size}\")\n\n    # show image in plot \n    plt.subplot(2, 5, i + 1)\n    plt.imshow(img_resized)\n    plt.title(isic_id)\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T14:39:10.359253Z","iopub.execute_input":"2025-06-29T14:39:10.359782Z","iopub.status.idle":"2025-06-29T14:39:10.931133Z","shell.execute_reply.started":"2025-06-29T14:39:10.359757Z","shell.execute_reply":"2025-06-29T14:39:10.930430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# transform the image size to the 64x64 resolution \nimage_size = 64\ntransform=transforms.Compose([\n                               transforms.Resize(image_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T14:39:13.433402Z","iopub.execute_input":"2025-06-29T14:39:13.433678Z","iopub.status.idle":"2025-06-29T14:39:13.438058Z","shell.execute_reply.started":"2025-06-29T14:39:13.433660Z","shell.execute_reply":"2025-06-29T14:39:13.437411Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 6. Make the dataloader and get the dataset\n\nFirst we get the data that are malignant.\n\nApply some transformation to lager the dataset.\n\nMake the dataset.","metadata":{}},{"cell_type":"code","source":"# data paths that contain images and other \ncsv_path = '/kaggle/working/train_created.csv'\nimage_dir = '/kaggle/input/isic-2024-challenge/train-image/image'\n\n# read the csv as data frame\ndf = pd.read_csv(csv_path)\n\n# custom dataset class \n# this implement how one training data is get from the dataset\n# if transformation is needed need to pass the transformation to transform\nclass ISICDataset(Dataset):\n    def __init__(self, df, image_dir, transform=None):\n        self.df = df\n        self.image_dir = image_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_path = os.path.join(self.image_dir, f\"{row['isic_id']}.jpg\")\n        image = Image.open(image_path).convert(\"RGB\")\n        \n        # resize and center crop using ImageOps.fit\n        image = ImageOps.fit(image, (128, 128), method=Image.BICUBIC)\n\n        if self.transform:\n            image = self.transform(image)\n\n        label = int(row['target'])\n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T14:39:17.637867Z","iopub.execute_input":"2025-06-29T14:39:17.638758Z","iopub.status.idle":"2025-06-29T14:39:17.742306Z","shell.execute_reply.started":"2025-06-29T14:39:17.638728Z","shell.execute_reply":"2025-06-29T14:39:17.741695Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# number of workers for dataloader\nworkers = 2\n\n# batch size during training\nbatch_size = 128\n\n# size using a transformer.\nimage_size = 64\n\n# number of channels in the training images. For color images this is 3\nnc = 3\n\n# size of latent space\nn_z = 300\n\n# learning rate \nlr = 0.0002\n\n# epochs \nepochs = 100","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T14:39:19.508101Z","iopub.execute_input":"2025-06-29T14:39:19.508651Z","iopub.status.idle":"2025-06-29T14:39:19.512762Z","shell.execute_reply.started":"2025-06-29T14:39:19.508629Z","shell.execute_reply":"2025-06-29T14:39:19.511939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data paths that contain images and other \ncsv_path = '/kaggle/working/train_created.csv'\n\n# read the csv as data frame\ndf = pd.read_csv(csv_path)\n  \n# generate sample dataset\ntrain_dataset = ISICDataset(df, image_dir, transform=transform)\n\n# create the dataloader\ndataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n                                         shuffle=True, num_workers=workers)\n\n# decide which device we want to run on\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n\n# plot some training images\nreal_batch = next(iter(dataloader))\nprint(\"Batch Size: \",real_batch[0].size())\nplt.figure(figsize=(12,12))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T14:39:23.027415Z","iopub.execute_input":"2025-06-29T14:39:23.028211Z","iopub.status.idle":"2025-06-29T14:39:26.688327Z","shell.execute_reply.started":"2025-06-29T14:39:23.028186Z","shell.execute_reply":"2025-06-29T14:39:26.687511Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 7. Make Encoder models \n\nEncoder Decoder architecture is used to generate new images.\n\nAfter training the model then it used to interpolate the malignant images and generate new synthetic images.","metadata":{}},{"cell_type":"code","source":"# create encoder module this is used to encode the image\nclass Encoder(nn.Module):\n    def __init__(self, n_channel, dim_h, n_z):\n        super(Encoder, self).__init__()\n\n        # number of input channels\n        self.n_channel = n_channel\n        # input image size \n        self.dim_h = dim_h\n        # latent space size\n        self.n_z = n_z\n        \n        # convolutional filters\n        # we use 5 convolution layers as our image is 64x64\n        self.conv = nn.Sequential(\n            \n            # 1 st convolution layer \n            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),\n            #nn.ReLU(True),(32 x 32 x 64)\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 2 nd convolution layer\n            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(self.dim_h * 2),\n            #nn.ReLU(True),(16 x 16 x 64*2)\n            nn.LeakyReLU(0.2, inplace=True),\n\n            # 3 rd convolution layer \n            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(self.dim_h * 4),\n            #nn.ReLU(True),(8 x 8 x 64*4)\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            \n            # 4 th convolution layer\n            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(self.dim_h * 8),\n            #nn.ReLU(True),(4 x 4 x 64*8)\n            nn.LeakyReLU(0.2, inplace=True),\n\n            # 5 th convolution layer\n            nn.Conv2d(self.dim_h * 8, self.dim_h * 16, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(self.dim_h * 16),\n            #nn.ReLU(True),(1 x 1 x 64*16)\n            nn.LeakyReLU(0.2, inplace=True)) \n\n        \n        # final layer is fully connected\n        self.fc = nn.Linear(self.dim_h * (2 ** 4), self.n_z)\n        \n\n    def forward(self, x):\n        # print('enc')\n        # input size is torch.Size([128, 3, 64, 64])\n        \n        x = self.conv(x)\n        # print(x.size()) # torch.Size([128, 1024, 1, 1])\n        \n        x = x.squeeze()\n        # print('aft squeeze ',x.size()) # torch.Size([128, 1024])\n        \n        \n        x = self.fc(x)\n        # print('out ',x.size()) #torch.Size([128, 320])\n        \n        # out  torch.Size([128, 300])\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T14:39:31.974109Z","iopub.execute_input":"2025-06-29T14:39:31.974458Z","iopub.status.idle":"2025-06-29T14:39:31.982410Z","shell.execute_reply.started":"2025-06-29T14:39:31.974431Z","shell.execute_reply":"2025-06-29T14:39:31.981725Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# debug cell\nsample_encoder = Encoder(nc, image_size, n_z)\n# sample forward pass\nencoded_output = sample_encoder(real_batch[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 7. Make Decoder models\n\nDecoder decode the latent space vector to generate new image","metadata":{}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, n_channel, dim_h, n_z):\n        super(Decoder, self).__init__()\n        \n        # number of input channels\n        self.n_channel = n_channel\n        # input image size \n        self.dim_h = dim_h\n        # latent space size\n        self.n_z = n_z\n\n        # first layer is fully connected\n        self.fc = nn.Sequential(\n            nn.Linear(self.n_z, self.dim_h * 8 * 8 * 8),\n            nn.ReLU())\n\n        # 4 deconvolutional filters as encoder has 5 convolution layers\n        self.deconv = nn.Sequential(\n            # 1 st deconvolution layer \n            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, 4, stride=2, padding=1),\n            nn.BatchNorm2d(self.dim_h * 4),\n            nn.ReLU(True),\n\n            # 2 nd deconvolution layer \n            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, 4, stride=2, padding=1),\n            nn.BatchNorm2d(self.dim_h * 2),\n            nn.ReLU(True),\n\n            # 3 rd deconvolution layer \n            nn.ConvTranspose2d(self.dim_h * 2, self.dim_h, 4, stride=2, padding=1),\n            nn.BatchNorm2d(self.dim_h),\n            nn.ReLU(True),\n\n            # 4 th deconvolution layer \n            nn.ConvTranspose2d(self.dim_h, 3, 3, stride=1, padding=1),\n            #nn.Sigmoid())\n            nn.Tanh())\n\n    def forward(self, x):\n        #print('dec')\n        # print('input ',x.size())\n        x = self.fc(x)\n        \n        # after the latent space make as channels to pass to the deconvolution layers\n        x = x.view(-1, self.dim_h * 8, 8, 8)\n        \n        # deconvolve the layer\n        x = self.deconv(x)\n\n        # output the image\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T14:39:36.077764Z","iopub.execute_input":"2025-06-29T14:39:36.078479Z","iopub.status.idle":"2025-06-29T14:39:36.085163Z","shell.execute_reply.started":"2025-06-29T14:39:36.078455Z","shell.execute_reply":"2025-06-29T14:39:36.084412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# debug cell\nsample_decoder = Decoder(nc, image_size, n_z)\n# sample forward pass\ndecoded_output = sample_decoder(encoded_output)\n# print the size of the data\nprint(decoded_output.size())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 8. Train the Encoder Decoder model to generate new sample data\n\nTrain the encoder decoder model to generate synthetic images.\n\nThere are few optimizing strategies to enhance the generated images.","metadata":{}},{"cell_type":"code","source":"# initialize encoder and decoder\nencoder = Encoder(nc, image_size, n_z)\ndecoder = Decoder(nc, image_size, n_z)\n\n# find available gpus\nt0 = time.time()\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)\n\n# add to the encoder and decoder to the device\ndecoder = decoder.to(device)\nencoder = encoder.to(device)\n\n# decoder loss function\ncriterion = nn.MSELoss()\ncriterion = criterion.to(device)\n\n# one optimizer with encoder and decoder\noptimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\n\n# add optimizers \nenc_optim = torch.optim.Adam(encoder.parameters(), lr=lr)\ndec_optim = torch.optim.Adam(decoder.parameters(), lr=lr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T14:39:42.145279Z","iopub.execute_input":"2025-06-29T14:39:42.145964Z","iopub.status.idle":"2025-06-29T14:39:42.341168Z","shell.execute_reply.started":"2025-06-29T14:39:42.145945Z","shell.execute_reply":"2025-06-29T14:39:42.340572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# debug cell\n# simple one step in training to check for the dimentions\n\nz_hat = encoder(real_batch[0])\nprint('zhat ', z_hat.size())       \nx_hat = decoder(z_hat) #decoder outputs tanh\nprint('xhat ', x_hat.size())\nmse = criterion(x_hat,real_batch[0])\nprint('mse ', mse)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 9. Make the simple training loop\n\nSimple training loop without any advance methods","metadata":{}},{"cell_type":"code","source":"# training loop\nfor epoch in range(epochs):\n    encoder.train()\n    decoder.train()\n    for images, _ in dataloader:\n        images = images.to(device)\n\n        # forward propagation \n        z = encoder(images)\n        x_recon = decoder(z)\n        loss = criterion(x_recon, images)\n\n        # backward propagation \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T14:39:49.459759Z","iopub.execute_input":"2025-06-29T14:39:49.460327Z","iopub.status.idle":"2025-06-29T15:00:49.924913Z","shell.execute_reply.started":"2025-06-29T14:39:49.460307Z","shell.execute_reply":"2025-06-29T15:00:49.924148Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 10. Generate the images with random noice \n\napply rondom noice in z latent space and generate images ","metadata":{}},{"cell_type":"code","source":"def generate_samples(decoder, n_samples=16, n_z=300, device='cuda'):\n    \n    # set to evaluation mode\n    decoder.eval()\n    \n    # sample latent vectors from standard normal\n    z = torch.randn(n_samples, n_z).to(device)\n\n    # generate images\n    with torch.no_grad():\n        generated = decoder(z).cpu()\n\n    # clamp and convert to [0,1] for viewing if using Tanh()\n    generated = (generated + 1) / 2.0\n\n    # plot the generated samples\n    grid_size = int(n_samples**0.5)\n    fig, axes = plt.subplots(grid_size, grid_size, figsize=(grid_size*2, grid_size*2))\n    for i, ax in enumerate(axes.flatten()):\n        img = generated[i].permute(1, 2, 0).numpy()  # CHW to HWC\n        ax.imshow(img)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T15:02:23.967322Z","iopub.execute_input":"2025-06-29T15:02:23.967712Z","iopub.status.idle":"2025-06-29T15:02:23.973996Z","shell.execute_reply.started":"2025-06-29T15:02:23.967685Z","shell.execute_reply":"2025-06-29T15:02:23.973318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save decoder model\ntorch.save(decoder.state_dict(), \"decoder.pth\")\n\n# Optionally save encoder too\ntorch.save(encoder.state_dict(), \"encoder.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T15:02:46.355257Z","iopub.execute_input":"2025-06-29T15:02:46.355592Z","iopub.status.idle":"2025-06-29T15:02:46.556255Z","shell.execute_reply.started":"2025-06-29T15:02:46.355570Z","shell.execute_reply":"2025-06-29T15:02:46.555650Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load trained decoder weights if needed\ndecoder.load_state_dict(torch.load(\"/kaggle/working/decoder.pth\", map_location=device))\n\ngenerate_samples(decoder, n_samples=16, n_z=n_z, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T15:03:04.161915Z","iopub.execute_input":"2025-06-29T15:03:04.162625Z","iopub.status.idle":"2025-06-29T15:03:04.918043Z","shell.execute_reply.started":"2025-06-29T15:03:04.162602Z","shell.execute_reply":"2025-06-29T15:03:04.917225Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"With the random noise it does not give better result.","metadata":{}},{"cell_type":"markdown","source":"### 11. First goes through the encoder and add small noice to it\n\nGenerate images are become more noice so add encoder sample and decode it.","metadata":{}},{"cell_type":"code","source":"def generate_from_encoded_noise(encoder, decoder, dataloader, device='cuda', noise_std=0.01, n_images=16):\n    encoder.eval()\n    decoder.eval()\n\n    with torch.no_grad():\n        for batch in dataloader:\n            images, _ = batch\n            images = images.to(device)\n\n            # Step 1: Encode\n            z = encoder(images)\n\n            # Step 2: Add small Gaussian noise\n            noise = torch.randn_like(z) * noise_std\n            z_noisy = z + noise\n\n            # Step 3: Decode\n            generated = decoder(z_noisy).cpu()\n\n            # Step 4: Normalize for viewing (assuming decoder uses Tanh)\n            generated = (generated + 1) / 2.0\n\n            # Step 5: Plot only first n_images\n            plt.figure(figsize=(12, 12))\n            for i in range(n_images):\n                plt.subplot(int(n_images**0.5), int(n_images**0.5), i + 1)\n                img = generated[i].permute(1, 2, 0).numpy()\n                plt.imshow(img)\n                plt.axis('off')\n            plt.tight_layout()\n            plt.show()\n            break  # only use first batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T15:14:26.456910Z","iopub.execute_input":"2025-06-29T15:14:26.457211Z","iopub.status.idle":"2025-06-29T15:14:26.463954Z","shell.execute_reply.started":"2025-06-29T15:14:26.457190Z","shell.execute_reply":"2025-06-29T15:14:26.463141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_from_encoded_noise(encoder, decoder, dataloader=dataloader, device=device, noise_std=0.005, n_images=16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T15:20:00.636216Z","iopub.execute_input":"2025-06-29T15:20:00.636582Z","iopub.status.idle":"2025-06-29T15:20:01.820668Z","shell.execute_reply.started":"2025-06-29T15:20:00.636549Z","shell.execute_reply":"2025-06-29T15:20:01.819934Z"}},"outputs":[],"execution_count":null}]}